import pandas as pd
import numpy as np
from gensim.models import KeyedVectors
import re
import torch
from torch.utils.data import Dataset


class CustomDataset(Dataset):
    def __init__(self, file_path):
        data = pd.read_table(file_path)
        # アルファベットのラベルを数字に
        self.labels = torch.tensor([0 if x == 'b' else 1 if x == 't' else 2 if x == 'e' else 3 for x in data['CATEGORY']])
        # ベクトル化
        model = KeyedVectors.load_word2vec_format('/Users/higuchitomoya/python/nlp100/chapter7/GoogleNews-vectors-negative300.bin.gz', binary=True)
        vectors = []
        for title in data['TITLE']:
            # タイトルを記号を取り除き小文字化し，単語に分割したものをベクトル化し，その平均を取る
            x = [model[word] for word in re.sub(r'[^a-zA-Z0-9]', ' ', title.lower()).split() if word in model]
            vectors.append(np.mean(x, axis=0))
        self.feature = torch.tensor(vectors)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return self.feature[idx], self.labels[idx]


if __name__ == "__main__":
    # 評価データについて試してみる
    test = CustomDataset("../chapter6/test.txt")
    print(test[0])

'''
(tensor([ 0.0163,  0.0173,  0.0235,  0.0381, -0.1697,  0.0821,  0.0289, -0.0734,
         0.0131,  0.0048, -0.0366, -0.1105, -0.0606,  0.0148, -0.0185,  0.0694,
         0.0649,  0.1155, -0.0189, -0.0948,  0.0507,  0.0357,  0.0279,  0.0080,
        -0.0490, -0.0206, -0.0899,  0.0992,  0.0640, -0.0455, -0.0404,  0.0514,
        -0.0891, -0.0414, -0.0421, -0.0447, -0.0444, -0.0032,  0.0068,  0.0020,
         0.0505, -0.0075,  0.0380,  0.0211,  0.0476,  0.0025,  0.0352, -0.0925,
         0.0324, -0.0782, -0.0392,  0.0352,  0.0138, -0.0111,  0.0579,  0.0148,
         0.0649, -0.0100,  0.0315, -0.0533, -0.1084, -0.0468, -0.0960, -0.0304,
        -0.0470,  0.0576, -0.0696,  0.0939, -0.0007,  0.0573, -0.0148,  0.0250,
         0.0935, -0.0372, -0.1347, -0.0342, -0.0209,  0.0612,  0.0559,  0.1509,
         0.0680, -0.0117,  0.0449,  0.0521,  0.0263, -0.0586, -0.0098,  0.0082,
         0.0532, -0.0022,  0.0349, -0.0272, -0.0705, -0.1106, -0.0787, -0.0130,
         0.0634,  0.0542,  0.0940,  0.0386, -0.0302, -0.1363,  0.0221, -0.0140,
         0.0418,  0.0120, -0.0166,  0.0289,  0.0223, -0.0841, -0.0088, -0.0403,
        -0.0880, -0.0511,  0.1309,  0.0329, -0.0205, -0.1299,  0.0937, -0.0027,
        -0.0008,  0.0811, -0.0599,  0.0797,  0.0023,  0.0123, -0.0814, -0.0085,
         0.1065,  0.0731, -0.1216, -0.0966, -0.1465, -0.0104,  0.0296, -0.0342,
         0.0418, -0.0309,  0.0587,  0.1214,  0.1640, -0.0303, -0.0299,  0.0550,
         0.0871, -0.0609,  0.0208, -0.0637, -0.0570, -0.0521,  0.0610, -0.0832,
        -0.1278,  0.0621, -0.0643, -0.0127, -0.0633, -0.0619, -0.0277,  0.0768,
         0.0382,  0.1001,  0.0105,  0.0719,  0.0396, -0.0519,  0.0274, -0.0224,
        -0.0270, -0.0178, -0.0917,  0.0076, -0.0034, -0.0702, -0.0507, -0.0116,
         0.1289, -0.1270, -0.0085,  0.0026, -0.0932, -0.0680, -0.0267, -0.0476,
        -0.0556, -0.0163, -0.0526, -0.0229,  0.0526,  0.0952, -0.0086,  0.0644,
         0.0047, -0.0349, -0.0852,  0.1567, -0.0830, -0.0016, -0.0432, -0.1262,
         0.0464,  0.0626, -0.0779,  0.0486,  0.0304, -0.0330, -0.0847, -0.1052,
         0.0765, -0.0029, -0.0499,  0.1463, -0.0483,  0.0746, -0.1016,  0.0168,
         0.0514,  0.0031, -0.1033,  0.0380, -0.0564,  0.0334, -0.0086,  0.0282,
         0.0331,  0.0432, -0.0119,  0.0342, -0.0205, -0.0062,  0.0510, -0.0311,
         0.0280,  0.0065,  0.0605, -0.1091, -0.0405, -0.0024,  0.0509,  0.0084,
         0.1520,  0.0392,  0.0160, -0.0830,  0.0379,  0.0529, -0.0285,  0.0722,
         0.0174, -0.1148, -0.0213, -0.1384,  0.0289,  0.0657,  0.0573,  0.0070,
         0.0610, -0.0442, -0.1070,  0.0713, -0.0013, -0.0520, -0.0277, -0.0479,
         0.1153,  0.1365,  0.0656, -0.1059, -0.0466,  0.0256, -0.0439,  0.1015,
         0.0551,  0.0536,  0.0494, -0.0565, -0.0172, -0.1029, -0.0390,  0.1107,
        -0.0398, -0.0059,  0.0157,  0.0880, -0.0590,  0.0441, -0.0383,  0.0275,
         0.0275, -0.0396, -0.0196,  0.0741, -0.1040,  0.0793,  0.0057, -0.0567,
         0.0449,  0.0043,  0.0861,  0.0211]), tensor(1))
'''
